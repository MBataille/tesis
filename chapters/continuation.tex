\chapter{Numerical Continuation}

\label{ch:continuation}

In the previous chapter, we mentioned the importance of finding and following
the solution branches as they experience several bifurcations. There, we considered simple
models where this information could be obtained analytically. However, in the problems considered
in this dissertation, and in general, it is not usually possible to do so. Therefore,
we must resort to numerical methods, more specifically, 
{\em numerical continuation} algorithms. This type of methods
aim to solve a nonlinear equation or, more generally, a system of nonlinear 
equations to find the desired steady states of a dynamical system 
as parameters are changed. This task corresponds to finding the roots 
(zeros) of a vector function $\bm{F}$, as in Eq.~(\ref{eq:pre_nc_zeros}). 
%It will be assumed that we previously know a solution $\bm{u}_0$ for a 
%certain parameter $\eta_0$, obtained for example through numerical simulation.

% This method  corresponds 

\begin{equation}
    0 = \bm{F}(\bm{u}, \eta)
    \label{eq:pre_nc_zeros}
\end{equation}

Although there are several methods for finding the roots of a vector function,
in this thesis we will only use Newton's method because of its fast 
(quadratic) convergence and simplicity. This method corresponds to an 
iterative algorithm that, given an initial guess $\bm{u_0}$, will perform 
successive iterations until a certain accuracy or tolerance is reached. 
Each iteration is computed using Eqs~(\ref{eq:pre_nc_newt1}-\ref{eq:pre_nc_newt2}),
where $\matr{J}(\bm{u}_i, \eta)$ is the Jacobian of $\bm{F}$. 
Moreover, since the Jacobian is computed at every point, the stability 
of the solution and the location of bifurcation points can be determined
by tracking the sign of the determinant of the Jacobian.

\begin{align}
    \matr{J}(\bm{u}_i, \eta) \Delta \bm{u}_{i+1} &= - \bm{F}(\bm{u}_i, \eta) 
    \label{eq:pre_nc_newt1}
    \\
    \bm{u}_{i+1} &= \bm{u}_i + \Delta \bm{u}_{i+1}
    \label{eq:pre_nc_newt2}
\end{align}

\section{Natural parameter continuation}

The simplest way to perform numerical continuation is to fix the value 
of the parameter, in this case $\eta$, and solve the equation (or system of 
equations) by means of Newton's method. Then, one can increase the parameter 
by a small step $\eta = \eta_0 + \Delta \eta$ and find the new solution using 
the previous solution $\bm{u_0}$ as initial guess for Newton's method. 
The process is repeated until the whole solution branch has been 
computed. This method is usually called {\em Natural Parameter Continuation}
\cite{doedel2007lecture}.

\begin{exmp}
    To illustrate the method, consider the normal form of the {\em imperfect pitchfork
    bifurcation}. Depending on the context, this model could describe magnetization
    under an external electric field or even optical bistability [refs]. We will 
    keep $\epsilon > 0$ fixed and
    find both the stable and unstable solution branches as $\eta$ is varied.
\begin{equation}
        \dot{u} = \eta + \varepsilon u - u^3
        \label{eq:pre_nc_she}
    \end{equation}

    This task corresponds to finding the roots of a cubic polynomial: $F(u, \eta) = \eta + \varepsilon u - u^3 = 0$. The derivative
    can be determined easily: $J(u, \eta) = \varepsilon - 3u^2$. Starting the algorithm
    at $\eta = -0.02$ with the initial guess $u_0=-0.4$ and moving forward (increasing $\eta$)
    yields the orange triangles shown in Fig.~\ref{fig:pre_nc_she}. Similarly, repeating
    the process backward (decreasing $\eta$) gives the green triangles shown in the same
    figure. 
    %Moreover, when $\eta=0$, $F$ admits three distinct real solutions when
    % $\varepsilon$ is positive and one real and two complex conjugate roots when $\varepsilon$ is negative. 

    % Fig~\ref{fig:pre_nc_she} shows the bistable case where $\varepsilon = 0.1$, 
    % Indeed, if we start the algorithm
    % at $\eta=-0.02$ taking as initial guess $u_0 = -0.4$ and perform a forward sweep (slowly increasing $\eta$), and then
    % repeat the process backwards we obtain Fig.~(\ref{fig:pre_nc_she}).

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.7\textwidth]{scripts/figures/natural_continuation_she.png}
        \caption[short]{Solution of Eq.~(\ref{eq:pre_nc_she}) as a function of the parameter $\eta$ obtained
        through natural continuation (orange and green triangles) compared to the exact solution (blue curve)
        with $\varepsilon = 0.1$.}
        \label{fig:pre_nc_she}
        %\includesvg[width=0.5\textwidth]{scripts/figures/natural_continuation_she.svg}
    \end{figure}
\end{exmp}

Note that in the example, the natural continuation succeeds at finding the lower and upper 
solution branches. However, it could not follow the branch past the fold 
(or saddle-node) bifurcation. The only way to access the middle branch using 
this algorithm would be to use an adequate initial guess close
to the middle branch. 
Although in this case, it is not difficult to find a good guess, 
for a higher-dimensional system where the
bifurcation scenario is more complicated, 
this quickly becomes impractical. To overcome this limitation,
one can implement a more robust continuation scheme: 
the {\em pseudo-arclength continuation}.

\section{Pseudo-arclength continuation}


As shown in the previous example, $\eta$ is not necessarily a good parameter to describe
the solution curve, as it does not follow the branch through folds. 
A different approach can be taken where the solution branch is parametrized
by a different variable: $s$, which is somewhat similar
to the arc-length. Therefore, our goal is to obtain a set of points 
$\bm{y}(s) = (\bm{u}(s), \eta(s))$.
The {\em pseudo-arclength} algorithm \cite{keller1977numerical} achieves this goal
through essentially two steps.
\begin{enumerate}
    \item {\em Predictor step.} Extrapolate a distance $\Delta s$ along the tangent $\bm{\tau}_0$ from a previously known point $(\bm{u}_0, \eta_0)$ in the $(\bm{u},\eta)$ space, to
    obtain the predicted point (the point used as initial guess). 
    $$\bm{y} = \bm{y}_0 + \bm{\tau}_0 \Delta s$$
    \item {\em Corrector step.} Force the solution to stay at a distance $\Delta s$ in the plane perpendicular to the tangent. 
    Or, equivalently, that the solution projected onto the tangent has length $\Delta s$. 
    $$(\bm{y} - \bm{y_0}) \cdot \bm{\tau}_0 = \Delta s$$
\end{enumerate}

In these steps, a new object has been introduced: the tangent $\bm{\tau}$ 
of the solution curve $\bm{y}(s)$ which is defined as follows.
\begin{equation}
    \bm{\tau} = \frac{d}{ds}\bm{y} = \left(\frac{d\bm{u}}{ds}, \frac{d\eta}{ds}\right)
\end{equation}

An additional step must therefore be carried out to implement this method: 
computing the tangent vector at each point. To do this, it is convenient to revisit 
Eq.~(\ref{eq:pre_nc_zeros}) and write out the dependence on the new 
parameter $s$ explicitly.
\begin{equation}
    0 = \bm{F}(\bm{u}(s), \eta (s))
\end{equation}

Taking the derivative with respect to $s$ on both sides of the
previous equation yields
\begin{equation}
    0 = \matr{J}(\bm{u}(s), \eta (s)) \frac{d \bm{u}}{ds} 
            + \bm{F_\eta}(\bm{u}(s), \eta (s)) \frac{d\eta}{ds}
   %  \matr{J}(\bm{x}(s), \eta (s))  \frac{d \bm{x}}{ds} &= -  \bm{F_\eta}(\bm{x}(s), \eta (s)) \frac{d\eta}{ds}
    \label{eq:pre_nc_tangent}
\end{equation}

Note that Eq.~(\ref{eq:pre_nc_tangent}) does not provide a unique solution for
the tangent vector. Consequently, another equation must be added by restricting
the length of the vector, more specifically, to normalize it.

\begin{equation}
    \left|\left|\dfrac{d\bm{u}}{ds}\right|\right|^2 + \left(\dfrac{d\eta}{ds}\right)^2 = 1
    \label{eq:pre_nc_tangent_normalization}
\end{equation}

Without loss of generality, one can fix the value of $\frac{d \eta}{ds} = 1$, solve 
Eq.~(\ref{eq:pre_nc_tangent}) for $\frac{d}{ds}\bm{u}$ and then normalize the obtained
vector $\tau$. Since Eq.~(\ref{eq:pre_nc_tangent}) is just a system of linear equations, 
it can be solved using a standard linear solver. It is important to mention that the
sign of $\tau$ must be chosen such that it has the same orientation as the 
previously known tangent $\bm{\tau}_0$ i.e. such that $\bm{\tau} \cdot \bm{\tau}_0 > 0$. 
In the very first step of the continuation method, the previous tangent is unkown. 
In that case, one can choose the orientation of $\bm{\tau}$ such that its last element 
(corresponding to  $\frac{d \eta}{ds}$) is positive to move forward (increasing $\eta$) or
negative to move backward (decreasing $\eta$).


To simplify the notation, it is convenient to define an extended vector 
function $\tilde{ \bm{F} }$ that incorporates $\bm{F}$ and the corrector 
step in the following manner,
\begin{equation}
    \tilde{\bm{F}}(\bm{y}) = 
    \begin{pmatrix}
        \bm{F}(\bm{y}) \\ 
        (\bm{y} - \bm{y}_0) \cdot \bm{\tau}_0 - \Delta s
    \end{pmatrix}.
    \label{eq:pre_nc_palc_system}
\end{equation} 

The corresponding extended Jacobian $\tilde{\matr{J}}$ becomes
\begin{equation}
    \tilde{\matr{J}} = 
    \begin{pmatrix}
        \matr{J} & \bm{F}_\eta \\
        \dfrac{d\bm{u}}{ds} & \dfrac{d\eta}{ds} \\
    \end{pmatrix}.
\end{equation}

Notice that the last row of the extended Jacobian $\tilde{\matr{J}}$ corresponds
exactly to the tangent vector $\bm{\tau}$. 

The pseudo-arclength continuation algorithm can be summarized in the following steps.

\begin{enumerate}
    \setcounter{enumi}{-1}
    \item Compute a first point in the solution branch $\bm{y}_0 = (\bm{u}_0, \eta_0)$,
    typically through direct numerical simulations. Additionally, one could run
    Newton's method once while keeping the parameter fixed at $\eta = \eta_0$ to obtain
    a more accurate approximation for $\bm{u}_0$.

    \item Solve Eq.~(\ref{eq:pre_nc_tangent}) and find the tangent at that point $\bm{\tau}_0$.
    Choose the orientation of $\bm{\tau}_0$ such that it points in the desired direction on the
    $\eta$-axis.

    \item Using $\bm{y}_0 + \bm{\tau}_0 \Delta s$ as initial guess in Newton's method, solve
    Eq.~(\ref{eq:pre_nc_palc_system}) to find the next point in the solution branch $\bm{y}_{i+1}$.

    \item Again, solve Eq.~(\ref{eq:pre_nc_tangent}) and find the tangent at that point $\bm{\tau}_{i+1}$.
    Choose the orientation such that it matches the previous tangent, $\bm{\tau}_{i+1} \cdot \bm{\tau}_i > 0$.

    \item Repeat steps 3-4 until the whole solution branch has been computed. One could also
    track changes in the sign of the determinant of $\matr{J}$ in order to estimate the location
    of bifurcation points.
\end{enumerate}

% \begin{defn}[ver \cite{KAR00}] Definición definitiva $$\frac{d}{dx}\int_a^xf(y)dy=f(x).$$\end{defn}

\begin{exmp}
    To illustrate the method, it is useful to revisit the previous example and implement the pseudo-arclength continuation
    to the same problem. The extended function $\tilde{\bm{F}}(u, {\eta})$ can be written
    in the following form,
    \begin{equation}
        \tilde{\bm{F}}(u, \eta) = 
            \begin{pmatrix}
                \eta + \varepsilon u - u^3 \\
                \left(u - u_0\right) \dfrac{du}{ds} + (\eta - \eta_0) \dfrac{d\eta}{ds} - \Delta s
            \end{pmatrix}.
            \label{eq:pre_nc_palc_she}
    \end{equation}
    Therefore, the extended Jacobian $\tilde{\matr{J}}$ reads,
    \begin{equation*}
        \tilde{\matr{J}} = \begin{pmatrix}
                \varepsilon - 3u^2 & 1 \\
                \dfrac{du}{ds} & \dfrac{d\eta}{ds}
            \end{pmatrix}.
    \end{equation*}

    The tangent vector $\bm{\tau} = (\tau_u, \tau_{\eta})$ can be computed by solving Eq.~(\ref{eq:pre_nc_tangent}).
    We start by fixing $\frac{d}{ds}\eta = 1$, then $\frac{d}{ds}u$ can be obtained directly,
    \begin{equation*}
        \dfrac{du}{ds} = -\dfrac{F_{\eta}}{J} = -\dfrac{1}{\varepsilon - 3u^2}.
    \end{equation*}
    Finally, we normalize $\bm{\tau}$ to obtain the tangent vector.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.7\textwidth]{scripts/figures/palc_she.png}
        \caption[short]{Solution of Eq.~(\ref{eq:pre_nc_palc_she}) as a function of the parameter $\eta$ obtained
        through the pseudo-arclength continuation (orange dots) compared to the exact solution (blue curve).}
        \label{fig:pre_palc_she}
        %\includesvg[width=0.5\textwidth]{scripts/figures/natural_continuation_she.svg}
    \end{figure}

\end{exmp}

\section{Continuation of traveling states}

\label{sec:cont_traveling}

In the particular case of following moving solutions with constant speed $c$, which is
the core of this work, some difficulties arise. The first and most evident one, is that
the desired solution is not steady anymore. This can be solved rather easily by
changing to the co-moving frame of reference, i.e. by inserting the traveling wave
ansatz $\bm{u}(x, t) = \bm{a}(x - ct)$, where $a$ is the solution profile in the co-moving frame,
 into Eq.~(\ref{eq:pre_nc_zeros}). Due to the chain rule,
an additional term in the form of a spatial derivative appears in the equation,

\begin{equation}
    0 = \bm{F}(\bm{a}, \eta) + c\partial_x\bm{a}
\end{equation}

The second problem is that usually the speed $c$ will change as parameters
are varied along the solution branch. Therefore, at
each step, the speed will have to be determined by the 
algorithm. The solution to this problem is to add the speed
as another unknown, that is to say, we will now be interested in solving for 
$\bm{y} = (\bm{a}, c, \eta)$. This leads to the third and final problem, which is that due
to the additional unknown, we are a missing an additional equation that will guarantee
a unique solution to the linearized system. Moreover, we will be solving these systems considering periodic
boundary conditions meaning that a translational invariance will appear. In order to
deal with the translational symmetry and guarantee a unique solution, a {\em phase condition}
or {\em pinning condition} must be established. Indeed, if we find a solution
$\bm{a}(x)$, then $\tilde{\bm{a}}_{\theta}(x) = \bm{a}(x  + \theta)$ is also
a solution for every $\theta$. 

The most widely used condition is the
{\em integral phase condition} \cite{doedel1981auto} which takes a reference solution $\bm{a}_0$ for a certain
parameter value $\eta_0$ close to the desired solution. The idea is to find the phase
that minimizes the difference $D$ between the desired solution $\bm{a}$ and the reference solution
$\bm{a}_0$. We can define the difference as follows,
\begin{equation}
    D(\theta) = \int_0^L dx' 
        \left|\left| \bm{a}(x' + \theta) - \bm{a}_0(x') \right|\right|^2    
\end{equation}

In order to minimize the difference, we differentiate the above equation, set it equal to zero and 
then integrate by parts. Thus, we arrive at the following condition which is simpler to
implement.

\begin{equation}
    p(\bm{a}, \bm{a}_0) = \int_0^L dx' \bm{a}(x') \cdot \left. \frac{d\bm{a}_0}{dx}\right\vert_{x'} = 0.
\end{equation}

We can re-define the extended vector function for which we want to find 
the root of in the following manner,

\begin{equation}
    \bm{H(\bm{y})} = \begin{pmatrix}
      \bm{F}(\bm{a}, \eta) + c\partial_x\bm{a} \\
    p(\bm{a}, \bm{a}_0) \\
    q(\bm{y}, \bm{y}_0)
    \end{pmatrix}
\end{equation}


The derivative of the integral phase condition with respect to the state vector $p_{\bm{a}}$ may differ depending on the chosen 
phase condition and implementation of the phase condition. In the simplest case, 
replacing the integral as a Riemann sum (which is the same as the trapezoidal rule in the case of periodic boundary conditions), the derivative reads

\begin{equation}
    p_{\bm{a}} = \Delta x \dfrac{d\bm{a}_0}{dx}.
\end{equation}

Therefore, the corresponding Jacobian of the extended function reads

\begin{equation}
    \matr{J_H}(\bm{y}) = \begin{pmatrix}
        \matr{J}(\bm{a}, \eta) + c\partial_x & \partial_x\bm{a} &  \bm{F}_\eta(\bm{a}, \eta)\\
        p_{\bm{a}}(\bm{a}_0) & 0 & 0 \\
        \dot{\bm{a}} & \dot{T} & \dot{\eta} \\
    \end{pmatrix}.
\end{equation}

Note that the last row corresponds, once again, exactly to the tangent vector
$\tau = (d\bm{u}/ds, dT/ds, d\eta/ds) = (\dot{\bm{u}}, \dot{T}, \dot{\eta})$.


\section{Continuation for periodic orbits}

If we know wish to follow periodic solutions with the continuation method we
must first derive a new set of equations to be solved. Mainly, periodic solutions
are not steady solutions of the differential equation, however they satisfy
the following boundary-value problem.

\begin{align}
    \dfrac{d\bm{u}}{dt} &= \bm{F}(\bm{u}, \eta) \\
    \bm{u}(t=0) &= \bm{u}(t=T)
\end{align}

It is convenient to rescale the time $t\to tT$, therefore the condition for
periodicity of the solution becomes $\bm{u}(t=0) = \bm{u}(t=1)$. Moreover,
due to the rescaling in time, a factor $T$ appears on the right-hand side of 
the dynamical equation. Therefore, the rescaled system becomes,

\begin{align}
    \dfrac{d\bm{u}}{dt} &= T\bm{F}(\bm{u}, \eta) \\
    \bm{u}(t=0) &= \bm{u}(t=1)
\end{align}

Moreover, due to the additional time-dependence of $\bm{u}$, the parametrizing equation for the pseudo-arclength
method needs to be modified accordingly. It now reads,

\begin{equation}
    q(\bm{y}, \bm{y}_0) = \int_0^1  (\bm{u}(t) -  \bm{u}_0(t)) \cdot \dfrac{d\bm{u}}{ds} dt 
        + (T - T_0) \dfrac{dT}{ds} + (\eta - \eta_0) \dfrac{d\eta}{ds} - \Delta s = 0 
\end{equation}

Additionally, one can add weights to the
previous equation in order to tune the search direction in Newton's method "horizontally"
(taking larger steps in the parameter $\eta$) or "vertically" (smaller steps in $\eta$),
see \cite{Uecker2017pde2path} for a more detailed discussion.

Note that we have introduced the period $T$ as another unknown which will be solved
through Newton's method along with $\bm{u}$ and $\eta$, i.e. we want to solve
for $\bm{y}(t) \equiv (\bm{u}(t), T, \eta)$. Moreover, as in the previous section, a phase condition
$p(\bm{u}, \bm{u_0}) = 0$ must be satisfied in order to deal with the translational invariance (in time) and
guarantee the uniqueness of the solution.
Finally, we can re-define the
extended vector function for which we want to find the root of in the following manner,

\begin{equation}
    \bm{H(\bm{y})} = \begin{pmatrix}
       T \bm{F}(\bm{u}, \eta) - \frac{d\bm{u}}{dt} \\
    p(\bm{u}, \bm{u}_0) \\
    q(\bm{y}, \bm{y}_0)
    \end{pmatrix}
\end{equation}


In order to solve this system of equations subject to periodic boundary conditions, many strategies can be followed. Namely, orthogonal collocation methods
(implemented in AUTO \cite{doedel1981auto}), multiple shooting methods \cite{keller1968numerical}, and last but not least, finite difference methods
(implemented in pde2path \cite{Uecker2017pde2path}). Although the latter has the least accuracy it is by far the 
simplest to implement. In the finite difference method, a possible approximation to the
first equation in the above system is the trapezoidal rule (used for instance in the Crank-Nicolson
scheme for simulating PDEs),

\begin{equation}
    \left( \dfrac{F(\bm{u}_i) + F(\bm{u}_{i+1})}{2} \right) T 
        - \dfrac{\bm{u}_{i+1} - \bm{u}_{i}}{t_{i+1} - t_i} = 0
\end{equation}

Note that the derivative of $\bm{u}$ with respect to $t$ can be written as a product
of a matrix $\matr{\nabla}_t$ and the time-discretized vector $\bm{u}_i$ $(1 \leq i \leq n_t)$, i.e.
it can be written as $\matr{\nabla}_t\bm{u}$, in which case the Jacobian $\matr{J_H}$ of $\bm{H}$
reads

\begin{equation}
    \matr{J_H}(\bm{y}) = \begin{pmatrix}
        T \matr{J}(\bm{u}, \eta) - \matr{\nabla}_t & \bm{F}(\bm{u}, \eta) &  T\bm{F}_\eta(\bm{u}, \eta)\\
        p_{\bm{u}}(\bm{u}_0) & 0 & 0 \\
        \dot{\bm{u}} & \dot{T} & \dot{\eta} \\
    \end{pmatrix}.
\end{equation}
